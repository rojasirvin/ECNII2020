---
title: "Evaluación experimental"
author: "Irvin Rojas"
institute: "CIDE"
date: "10 de noviembre de 2020"
mathspec: true
header-includes:
  - \usepackage{amsmath}
  - \usepackage{commath}
output:
  xaringan::moon_reader:
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
      scroll: false
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
include-before:
- '\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}'

---
class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")

library(tidyverse)
library(pacman)
library(janitor)
library(sandwich)
library(modelsummary)
#library(nnet)
#library(mlogit)
#library(quantreg)
library(FNN)
library(np)

p_load(tidyverse, foreign, reshape2, psych, qwraps2, forcats, readxl, 
       broom, lmtest, margins, plm, rdrobust, multiwayvcov,
       wesanderson, sandwich, stargazer,
       readstata13, pscore, optmatch, kdensity, MatchIt, bootstrap, matlib, dplyr)

xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))

```

```{css, echo = FALSE}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}

```


.title[
# Sesión 25. Evaluación experimental
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas <br> División de Economía
]

---

# Motivación

- Hemos trabajado hasta ahora modelando las distribuciones de variables aleatorias o la media condicional

- Muchos problemas nos dan cierta estructura (restricciones de homogeneidad, por ejemplo) que nos hacen asumir ciertos modelos

- En otras ocasiones, podemos analizar los datos sin imponer supuestos distribucionales y dejar *que los datos hablen*

- Los métodos no paramétricos son buenos aliados para la representación de relaciones

- Frecuentemente complementamos análisis paramétricos con sus contrapartes no paramétricas

---

class: inverse, middle, center

# Estimación de densidades

---

# Histograma

- Consideremos una variable aleatoria continua $x$

- Un **histograma** es un estimador de la densidad formado al partir el rango de $x$ en intervalos iguales y calculando la fracción de dados en cada intervalo

- Queremos estimar la densidad $f(x_0)$, es decir, al densidad de $x$ en $x_0$

- Un histograma está definido por

$$\hat{f}_{hist}(x_0)=\frac{1}{N}\sum_i\frac{\mathbf{1}(x_0-h<x_i<x_0+h)}{2h}$$

--

- A $2h$ se le llama el ancho de ventana

- El histograma pesa a las observaciones que están dentro el rango dado por $(-h,h)$

---

# Histograma

- Este peso igual queda claro al reescribir el histograma como

$$\hat{f}_{hist}(x_0)=\frac{1}{Nh}\sum_i\frac{1}{2}\mathbf{1}\left(\Bigg|\frac{x_i-x_0}{h}\Bigg|<1\right)$$
- Usemos datos sobre salarios de 175 mujeres para ejemplificar la construcción del histograma

- Usamos 20 ventanas


---

# Ejemplo: histograma

.pull-left[
```{r echo=T, include=T, eval=T, results=T, message=F, warning=F}

data.salarios<-read_csv("./salarios.csv",
                        locale = locale(encoding = "latin1"))

h1 <- data.salarios %>% 
  ggplot(aes(x=lnhwage)) +
    geom_histogram(aes(y=..density..),
                   bins=20,
                   fill="#69b3a2",
                   color="#e9ecef",
                   alpha=0.9) +
    ggtitle("Histograma del log del salario por horar") +
    theme(plot.title = element_text(size=15))

```
]

.pull-right[
```{r out.width="100%", echo=F, include=T, eval=T, results=T, message=F, warning=F}
h1
```
]
---

# Densidad Kernel

- El estimador de densidad Kernel (Rosenblatt, 1956) es la generalización del histograma

- Definimos la densidad estimada Kernel como

$$\hat{k}(x_0)=\frac{1}{Nh}\sum_i \mathit{K}\left(\frac{x_i-x_0}{h}\right)$$
- $\mathit{K}$ es una función de pesos Kernel

- $h$ es el ancho de banda o parámetro de suavizaición

- La función Kernel evalúa más datos alrededor de $x_0$ que el histograma (posiblemente todos los datos)

---

# Función Kernel

- La función $\mathit{K}$ cumple con

  1. $\mathit{K}(z)$ es simétrica alrededor de 0 y es continua
  
  1. $\int\mathit{K}(z)dz=1$, $\int z\mathit{K}(z)dz=0$ y $\int|\mathit{K}(z)|dz<\infty$
  
  1. i) $K(z)=0$ si $|z|\geq z_0$ para algún  $z_0$, o ii) $|z|K(z)\to 0$ si $|z|\to\infty$
  
  1. $\int z^2 \mathit{K}(z)dz=\kappa$, donde $\kappa$ es una constante

--

- Una función que satisface estas condiciones puede ser una función Kernel para pesar las observaciones y estimar la densidad en $x_0$

- Al proveer $\mathit{K}$ y $h$, es relativamente fácil obtener el estimador de la densidad

---

# Funciones Kernel comúnmente usadas

| Kernel | $\mathit{K}$ |
|:---|:---:|
| Uniforme, *box* o rectangular | $\frac{1}{2} \mathbf{1}(abs(z)<1)$ |
| Triangular | $(1-abs(z))\mathbf{1}(abs(z)<1)$ |
| Epanechnikov o cuadrático | $\frac{3}{4}(1-z^2)\mathbf{1}(abs(z)<1)$ |
| Normal o gausiano | $\frac{1}{\sqrt{2\pi}}exp(-z^2/2)$ |


---

# Elección del ancho de banda

- Un ancho de banda pequeño reduce el sesgo (usamos observaciones muy parecidas, aunque pocas)

- Un ancho de banda grande mejora la suavización

- El ancho de banda es una decisión más importante que la elección del tipo de Kernel

--

- Consideremos el error cuadrático medio como una medida de desempeño

$$MSE(\hat{f}(x_0))=E\left(\hat{f}
(x_0)-f(x_0)\right) ^2$$

- Una medida global de qué tan buena es la aproximación es el **error cuadrado promedio integrado**

$$MISE(h)=\int MSE(\hat{f}(x_0))dx_0$$
---

# Elección del ancho de banda

- Silverman (1986) muestra que

$$h^*=\delta \left(\int f'' (x_0)^2dx_0\right)^{-0.2}N^{-0.2}$$
donde $\delta=\left(\frac{\int \mathit{K}(z)^2dz}{(\int z^2\mathit{K}(z)dz})^2\right)$

--

- Noten que $h^*$ depende de la curvatura de la densidad

- Si $f(x)$ es muy variable, $h$ será pequeña


---

# Estimación Kernel en la práctica

- Usamos $\delta$ estimador por Silverman, dependiendo del Kernel elegido

| Kernel | $\mathit{K}$ | $\delta$ |
|:---|:---:|:---:|
|Uniforme, *box* o rectangular| $\frac{1}{2}\mathbf{1}(abs(z)<1)$ |  1.3510 |
| Triangular | $(1-abs(z))\mathbf{1}(abs(z)<1)$ | - |
| Epanechnikov o cuadrático | $\frac{3}{4}(1-z^2)\mathbf{1}(abs(z)<1)$ | 1.7188 |
| Normal o gausiano | $\frac{1}{\sqrt{2\pi}}exp(-z^2/2)$ | 0.7764 |

--

- El estimador *plug-in* de Silverman funciona casi siempre

$$h^*_{Silverman}=1.3643\delta\left(\min \left\{s,\frac{iqr}{1.349}\right\}\right)N^{-0.2}$$
donde $s$ es la desviación estándar de los datos, $iqr$ es el rango intercuartil y $\frac{iqr}{1.349}$ protege en presencia de observaciones atípicas




---

# Ejemplo: densidad Kernel estimada

- Estimamos un Kernel epanechnikov con el ancho de banda óptimo

- Primero calculemos el ancho de banda óptimo

- Usamos $\delta$ calculado por Silverman

```{r echo=T, include=T, eval=T, results=T, message=F, warning=F}
delta <- 1.7188 # Ver CT
w.sd <- sd(data.salarios$lnhwage)
w.iqr.adj <- IQR(data.salarios$lnhwage)/1.349
w.N <- nrow(data.salarios)
constante <- 1.3643
ajuste <- min(w.sd,w.iqr.adj)
h <- constante*delta*ajuste*w.N^(-0.2) # ancho de banda
h
```




---

# Ejemplo: densidad Kernel estimada

.pull-left[
- El parámetro *bw* en *geom_density* especifica el ancho de banda

- Pero en R, el ancho de banda es lo que para nosotros es el ancho de ventana

- Debemos de especificar la mitad de $h$

```{r echo=T, include=T, eval=T, results=T, message=T, warning=F}
k1 <- data.salarios %>% 
  ggplot(aes(x=lnhwage)) +
    geom_histogram(aes(y=..density..), bins=20, fill="#69b3a2", color="#e9ecef", alpha=0.9) +
    geom_density(kernel="epanechnikov", bw=h/2 , n=50)+
    theme(plot.title = element_text(size=15))
```
]

.pull-right[
```{r out.width="100%", echo=F, include=T, eval=T, results=T, message=F, warning=F}
k1
```
]

---

# Ejemplo: densidad Kernel estimada

.pull-left[

- Vemos cómo el ancho de banda seleccionado modifica el Kernel estimado

```{r echo=T, include=T, eval=T, results=T, message=T, warning=F}
k2 <- data.salarios %>% 
  ggplot(aes(x=lnhwage)) +
    geom_histogram(aes(y=..density..), bins=20, fill="#69b3a2", color="#e9ecef", alpha=0.9)+
    geom_density(aes(x=lnhwage, color='Óptimo'), kernel="epanechnikov", bw=h/2, adjust=1)+
    geom_density(aes(x=lnhwage, color='1/2 óptimo'), kernel="epanechnikov", bw=h/2, adjust=0.5)+
    geom_density(aes(x=lnhwage, color='2 óptimo'), kernel="epanechnikov", bw=h/2, adjust=2)+
    theme(legend.position = 'right')+
    scale_color_manual("h",values = c('Óptimo' = 'black', '1/2 óptimo' = 'red', '2 óptimo'='blue'))
```
]

.pull-right[
```{r out.width="100%", echo=F, include=T, eval=T, results=T, message=F, warning=F}
k2
```
]

---

# Ejemplo: densidad Kernel estimada

.pull-left[

- Notamos que la elección del tipo de Kernel es menos relevante

```{r echo=T, include=T, eval=T, results=T, message=T, warning=F}
delta.gausiano <- 0.7764
delta.uniforme <- 1.3510
delta.cuartico <- 2.0362

h.gausiano <- constante*delta.gausiano*ajuste*w.N^(-0.2)
h.uniforme <- constante*delta.uniforme*ajuste*w.N^(-0.2)
h.cuartico <- constante*delta.cuartico*ajuste*w.N^(-0.2)

k3 <- data.salarios %>% 
  ggplot(aes(x=lnhwage)) +
    geom_histogram(aes(y=..density..), bins=20, fill="#69b3a2", color="#e9ecef", alpha=0.9)+
    geom_density(aes(x=lnhwage, color='epanechnikov'), kernel="epanechnikov", bw=h/2, adjust=1)+
    geom_density(aes(x=lnhwage, color='gausiano'), kernel="gaussian", bw=h.gausiano/2, adjust=1)+   
    geom_density(aes(x=lnhwage, color='uniforme'), kernel="rectangular", bw=h.uniforme/2, adjust=1)+
    geom_density(aes(x=lnhwage, color='cuártico'), kernel="biweight", bw=h.cuartico/2, adjust=1)+
    theme(legend.position = 'right')+
    scale_color_manual("Kernel", values = c('epanechnikov' = 'black', 'gausiano' = 'red', 'uniforme'='blue', 'cuártico'='green'))
```
]

.pull-right[
```{r out.width="100%", echo=F, include=T, eval=T, results=T, message=F, warning=F}
k3
```
]


---

class: inverse, middle, center

# Regresión local no paramétrica

---

# Regresión local no paramétrica

- Consideremos el siguiente modelo con un regresor escalar

$$y_i=m(x_i)+\varepsilon_i$$

- Tenemos $N$ observaciones y $\varepsilon_i\sim iid(0,\sigma^2_{\varepsilon})$

- Queremos usar las $x$ para decir algo sobre $y$ pero no queremos darle un modelo paramétrico

---

# Promedios locales ponderados

- Supongamos que para un valor de $x$, digamos $x_0$, observamos $N_0$ valores de $y$

- Una forma de estimar $m(x_0)$ es con la media muestral $\tilde{x}(x_0)$

- El problema es que este estimador puede ser muy ruidoso con regresores discretos y muestras pequeñas

--

- Una alternativa es mirar en una vecindad de $x_0$


- Definimos un **estimador de medias ponderadas local**

$$\hat{m}(x_0)=\sum_{i=1}^Nw_{i0,h}y_i$$

- Los pesos $w_{i0,h}$, con $\sum_i w_{i0,h}=1$ indican cuánto pesan las observaciones alrededor de $x_0$, dándole más peso a las más cercanas

- $h$ es el ancho de ventana

---

# Promedios locales ponderados

- Si decidimos que cada observación en la vecindad dada por $h$, el estimador local es

$$\hat{m}(x_0)=\frac{\sum_i\mathbf{1}\left(\Bigg|\frac{x_i-x_0}{h}\Bigg|<1\right)y_i}{\sum_i\mathbf{1}\left(\Bigg|\frac{x_i-x_0}{h}\Bigg|<1\right)}$$
- El numerador es la suma de $y_i$ para las observaciones dentro del ancho de banda

- El denominador es el número de unidades sobre las que se está sumando

---

# Promedios locales ponderados

- De hecho, las predicciones de MCO también pueden expresarse como un estimador de medias ponderadas

- Algo de álgebra resulta en

$$\hat{m}_{MCO}(x_0)=\sum_i\left\{\frac{1}{N}+\frac{(x_0-\bar{x})(x_i-\bar{x})}{\sum_j (x_j-\bar{x})^2}\right\}y_i$$
---

# Estimador de vecinos más cercanos

- Consideremos una vecindad de tamaño $k$, donde tenemos $(k-1)/2$ observaciones con $x<x_0$ y otras tantas para $x>x_0$

- Entonces, el estimador de **k vecinos más cercanos**, de **media móvil** o de **media móvil local**

$$\hat{m}_{k-NN}(x_0)=\frac{1}{k}\sum_i \mathbf{1}(x_i\in N_k(x_0))y_i$$
- Aquí $k$ juega el papel de $h$

- $h$ es variable: $h_0$ es igual a la distancia entre $x_0$ y el más lejano de los $k$ vecinos más cercanos

- A $k/N$ se le conoce como *lapso* o *span*

---

# Regresión kernel

- Una extensión natural el estimador de promedios locales ponderados es usar una función Kernel para pesar más las observaciones más cercanas a $x_0$

- Esto resulta en el **estimador de regresión Kernel**

$$\hat{m}(x_0)=\frac{\frac{1}{Nh}\sum_i\mathit{K}\left(\frac{x_i-x_0}{h}\right)y_i}{\frac{1}{Nh}\sum_i\mathit{K}\left(\frac{x_i-x_0}{h}\right)}$$
---

# Regresión local lineal

- Podemos asumir que $m(x)$ es lineal en la vecindad de $x_0$

$$m(x)=a_0+b_o(x-x_0)$$
- El **estimador de regresión local lineal** se obtiene al esocger $a_0$ y $b_0$ que resuelven resolver

$$\sum_i \mathit{K}\left(\frac{x_i-x_0}{h}\right)(y_i-a_0-b_0(x_i-x_0))^2$$

- Obtenemos $\hat{m}(x)=\hat{a}_0+\hat{b}_0(x-x_0)$

--

- En general, podemos asumir un polinomio de orden $p$ para $m(x)$ y resolver


$$\sum_i \mathit{K}\left(\frac{x_i-x_0}{h}\right)\left(y_i-a_{0,0}-a_{0,1}(x_i-x_0)-\ldots-a_{0,p}\frac{(x_i-x_0)^p}{p!}\right)^2$$
---

# Estimador Lowess

- Un estimador comúnmente usado es el **estimador suavizador de puntos con pesos locales**  (*locally weighted scatterplot smoothing estimator*) o **Lowess**

- Caso especial del estimador polinomial

  - Kernel tricúbico, $\mathit{K}=(70/81)(1-abs(z)^3)^3\mathbf{1}(abs(z)<1)$

  - Otorga menor peso a observaciones con residuales grandes
  
--

- Resulta robusto a la presencia de observaciones atípicas

- Es intensivo en cómputo


---

# Ejemplo: regresión no paramétrica

- Simulamos un proceso

```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}
N <- 100
u <- rnorm(n=N, mean=0, sd=25)
x <- seq(1:N)
y <- 150 + 6.5*x -0.15*x^2+0.001*x^3+u

data.sim <- as.data.frame(cbind(x,y))
```


- Usamos la función *knn.reg* de la paquetería *FNN*

```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}

knn5 <- knn.reg(data.sim$x, y=data.sim$y, k=5)
knn25 <- knn.reg(data.sim$x, y=data.sim$y, k=25)

data.sim <- data.sim %>% 
  mutate(y5=knn5$pred) %>% 
  mutate(y25=knn25$pred)

```
---

# Ejemplo: regresión no paramétrica

- Notamos que $h$ más grande genera una función más suave

.pull-left[
```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}
np1 <- data.sim %>% 
  ggplot(aes(x=x,y=y))+
  geom_point()+
  geom_smooth(aes(x=x,y=y, color='MCO'), method=lm, se=FALSE)+
  geom_line(aes(x=x,y=y5, color='kNN, k=5'))+
  geom_line(aes(x=x,y=y25, color='kNN, k=25'))+
  theme(legend.position = 'right')+
  scale_color_manual("Método", values = c('kNN, k=5' = 'blue', 'kNN, k=25'='green', 'MCO'='black'))

```
]

.pull-right[
```{r out.width="100%", echo=F, include=T, eval=T, results=T, message=F, warning=F}
np1
```

]

---

# Ejemplo: regresión no paramétrica

- Y ahora incluimos la función generada por Lowess

.pull-left[
```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}

fit.lowess <- lowess(data.sim$x, y=data.sim$y, f=25/100)

data.sim <- data.sim %>% 
  mutate(y.lowess=fit.lowess$y)
  
np2 <- data.sim %>% 
  ggplot(aes(x=x,y=y))+
  geom_point()+
  geom_smooth(aes(x=x,y=y, color='MCO cúbico'), method=lm, formula= y ~ x+I(x^2)+I(x^3), se=FALSE)+
  geom_line(aes(x=x,y=y.lowess, color='Lowess k=25'))+    theme(legend.position = 'right')+
  scale_color_manual("Método", values = c('MCO cúbico' = 'blue', 'Lowess k=25' = 'red'))


```
]

.pull-right[
```{r out.width="100%", echo=F, include=T, eval=T, results=T, message=F, warning=F}
np2
```

]

---

class: inverse, middle, center

# Regresión semiparamétrica


---

# Regresión semiparamétrica

- La teoría económica puede informar sobre la estructura de problemas y queremos introducir esta estructura en los modelos

- Con muchos regresores, la estimación no paramétrica se vuelve impráctica

- Podemos usar modelos *híbridos* que tengan una parte paramétrica y otra no paramétrica

- Algunos de estos modelos son más usados que otros en microeconometría


---

# Modelos lineal parcial

- Especificamos

$$E(y|x,z)=x'\beta+\lambda(z)$$

donde $\lambda(cdot)$ es una función escalar no especificada

- Por ejemplo, la demanda de cierto bien puede ser lineal en $x$ y no lineal en $z$

- El modelo de regresión es

$$y_i=x_i'\beta+\lambda(z_i)+\varepsilon_i \\ E(\varepsilon_i | x_i,z_i)=0 $$
---

# Transformación de Robinson

- Robinson (1988) propone lo siguiente

  1. Aplicando el operador de esperanza y por la ley de expectativas iteradas
  
  $$E(y_i|z_i)=E(x_i'\beta|z_i)+E(\lambda(z_i)|z_i)+E(\varepsilon_i|z_i) = E(x_i|z_i)'\beta+\lambda(z_i)$$

  1. Restando a la ecuación de $y_i$ original
  
  $$y_i-E(y_i|z_i)=(x_i-E(x_i|z_i))'\beta+\varepsilon_i$$

---

# Estimador de Robinson

- El **estimador de Robinson** consiste en hacer MCO a la ecuación transformada

$$y_i-\hat{m}_{yi}=(x-\hat{m}_{xi})'\beta_{LP}+\nu$$

--

- $\hat{m}_{yi}$ y $\hat{m}_{xi}$ son predicciones obtenidas por regresiones no paramétricas de $y$ y $x$ sobre $z$

- Robinson (1988) muestra que el estimador LP es consistente y asintóticamente normal

- Ver CT, Sección 9.7.3, con las expresiones de la varianza del estimador de Robinson

---

# Ejemplo: estimador de Robinson

- Usamos la función *npplreg* de la paquetería *np*

- Usaremos una base de datos que viene con *np* con 524 observaciones

  - Tenemos datos de edad, raza, salarios, educación, experiencia

```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}
data(wage1)
colnames(wage1)
```
---

# Ejemplo: estimador de Robinson

.pull-left[
- Propongamos un modelo donde el salario por hora depende no paramétricamente de la experiencia

$$\ln w_i= \beta_0+\beta_1 educ_i + \beta_2 tenure_i + \lambda(exper_i) $$

- La función *npplregbw* nos permite obtener los anchos de banda apropiados para cada variable

```{r out.width="100%", echo=T, include=T, eval=T, results=F, message=F, warning=F}
bw <- npplregbw(formula=lwage~educ+ tenure | exper, data=wage1)
```

- Pasamos el objeto *bw* a la función *npplreg*
```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}
model.pl <- npplreg(bw)
```

]

.pull-right[
```{r out.width="100%", echo=T, include=T, eval=T, results=T, message=F, warning=F}
summary(model.pl)
```
]

---

# Ejemplo: estimador de Robinson

.pull-left[

- Graficamos resultados
```{r out.width="100%", echo=T, include=T, eval=T, results=F, message=F, warning=F}
g.robinson <- npplot(bw)
```
]

---

# Próxima sesión

- En la siguiente clase hablaremos sobre los conceptos básicos de evaluación experimental para estimar efectos de tratamiento

  - CT, Capítulo 25
---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**
