---
title: "Pruebas de hipótesis"
author: "Irvin Rojas"
institute: "CIDE"
date: "27 de agosto de 2020"
mathspec: true
output:
  xaringan::moon_reader:
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
      scroll: false
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
include-before:
- '\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{s}}}{~}}'

---
class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")
library(tidyverse)
library(magick)
library(reticulate)

xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))
```

.title[
# Sesión 4. Pruebas de hipótesis
]
.subtitle[
## Econometría II
]
.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas <br> División de Economía
]

---
# Agenda
  
1. Estudiaremos los tests más usados para probar restricciones no lineales

1. Elaboraremos brevemente sobre sus propiedades asintóticas

1. Daremos algunos ejemplo de tests usados para probar algunas hipótesis con las que comúnmente nos topamos en el trabajo aplicado


---

class: inverse, middle, center

# Test de Wald

---

# Test de Wald

- Es uno de los más usados en pruebas de hipótesis en econometría

- En Econometría I seguramente vieron la versión lineal de este test (quizás sin el nombre) cuando querían probar restricciones lineales en un modelo lineal

--

- En un modelo lineal con $y=X'\beta+u$ y con $K=4$, queremos probar la hipótesis de que $\beta_1=1$ y $\beta_2-\beta_3=2$

- $h$ es el número de hipótesis a probar (en este caso 2)

- $R$ es una matriz de $h$ x $K$

- Tenemos una hipótesis nula y una alternativa:

$$
\begin{aligned}
H_0: R\beta_0-r=0 \\
H_a: R\beta_0-r\neq 0
\end{aligned}
$$
---

# Test de Wald

- En este caso, la matriz $R$ tiene la forma:

$$
R=
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & -1 &  0 \\
\end{pmatrix}
$$
- Mientras que $r=\begin{pmatrix} 1 \\ 2 \\ \end{pmatrix}$

--

- El test de Wald de $R\beta_0-r=0$ se basa en qué tan cercano a cero es el análogo muestral $R\hat{\beta}-r=0$, donde $\hat{\beta}$ es el estimador de MCO

--

- Supongamos que $u\sim\mathcal{N}(0,\sigma^2I)$, por lo que $\hat{\beta}\sim\mathcal{N}(\beta_0,\sigma^2_0(X'X)^{-1})$

- Entonces, $R\hat{\beta}-r\sim\mathcal{N}(0,\sigma^2_0R(X'X)^{-1}R')$


---

# Test de Wald

- Podemos entonces construir la forma cuadrática $W_1$:

$$W_1=(R\hat{\beta}-r)'\left(\sigma_o^2R(X'X)^{-1}R'\right)^{-1}(R\hat{\beta}-r)$$
- Que se puede mostrar que tiene una distribución $\chi^2(h)$

--

- En la práctica, no observamos $W_1$, así que usamos la versión con la varianza estimada:

$$W_2=(R\hat{\beta}-r)'\left(\hat{s}^2R(X'X)^{-1}R'\right)^{-1}(R\hat{\beta}-r)$$
- $W_2$ tiene una distribución asintítica $\chi^2(h)$ bajo la $H0$


- Podemos calcular $W_2$ y la probabilidad de que dicho valor ocurra en una distribución $\chi^2(h)$

- Si la probabilidad es menor al nivel de significancia elegido (típicamente 5% en economía), se rechaza la $H0$

--

- Problema: test muy restrictivo pues depende del supueto de homocedasticidad

---

class: inverse, middle, center


# Hipótesis no lineales

---

# Hipótesis no lineales

- Consideremos $h$ restricciones, posiblemente no lineales en los parámetros

- El vector de parámetros es de dimensión $q\times 1$ con $h\leq q$

--

- Queremos probar:

$$
\begin{aligned}
H_0: h(\theta_0)=0 \\
H_a: h(\theta_0)\neq 0
\end{aligned}
$$

---

# Test de Wald no lineal

- El estadístico de Wald es:

$$W_{NL}=\hat{h}'(\hat{R}\hat{V}(\hat{\theta})\hat{R}')\hat{h}$$

con $\hat{h}=h(\hat{\theta})$ y con $\hat{R}=\frac{\partial h(\hat{\theta})}{\partial \theta'}\Bigg|_{\hat{\theta}}$

- $W_{NL}$ se distribuye asintóticamente como $\chi^2(h)$ bajo la $H0$

- Rechazamos $H0$ en favor de $Ha$ a un nivel de significancia $\alpha$ si $W_{NL}>\chi^2_{\alpha}(h)$

- O, equivalentemente, rechazamos $H0$ a un nivel $\alpha$ si el valor $p$, es decir $P(\chi^2(h)>W)$ es menor que $\alpha$

---

# Derivación del estadístico de Wald no lineal


- Consideremos la restricción $h(\hat{\theta})$

- Una expansión de Taylor alrededor de $\theta_0$ resulta en:

$$h(\hat{\theta})=h(\theta_0)+\frac{\partial h(\theta)}{\partial \theta'}\Bigg|_{\theta^+}(\hat{\theta}-\theta_0)$$
- Como ya hemos hecho antes, podemos reescalar y resolver:

$$\sqrt{N}(h(\hat{\theta})-h(\theta_0))=R(\theta^+)\sqrt{N}(\hat{\theta}-\theta_0)$$
donde, en adelante, $R(\theta)=\frac{\partial h(\theta)}{\partial \theta}$

---

# Derivación del estadístico de Wald no lineal

- Si podemos aplicar una LGN y un TLC, sabemos la distribución límite del lado izquierdo:


$$\sqrt{N}(h(\hat{\theta})-h(\theta_0))=\xrightarrow{d}\mathcal{N}(0,R_0C_0R_0')$$
con $C_0=V(\hat{\theta})$

- Y bajo $H0$:

$$\sqrt{N}h(\hat{\theta})=\xrightarrow{d}\mathcal{N}(0,R_0C_0R_0')$$
---

# Derivación del estadístico de Wald no lineal
- Recordemos que si $z\sim\mathcal{N}(0,\Omega)$, entonces $z'\Omega^-1z\sim\chi^2(dim(\Omega))$

- Entonces 

$$h(\hat{\theta})'\left(R_0C_0R_0'\right)^{-1}h(\hat{\theta}) \xrightarrow{d}\chi^2(h)$$
- El estadístico $W_{NL}$ se obtiene reemplazando $R_0$ y $C_0$ por estimadores consistentes

---

class: inverse, middle, center


# Usos del test de Wald

---

# Test de significancia

- Un test de significancia se usa para probar si $\theta_j$ es distinto de cero

- Entonces $h(\theta)=\theta_j$

- El vector $r(\theta)=\frac{\partial h}{\partial \theta'}$ es un vector de ceros, excepto en la $j$ésima entrada que toma el valor de 1

--

- En este caso, el estadístico de Wald queda como:

$$W_z=\frac{\hat{\theta}}{se(\hat{\theta}_j)}$$
- Un objeto familiar para ustedes, comúnmente llamdo estadístico $t$, aunque estrictamente, se distribuye asintóticamente como una normal (de hecho, normal estándar, de ahí su nombre $z$)

---

# Restricciones de exclusión

- Queremos probar que los últimos $h$ componentes de $\theta$ son 0

- Partimos $\theta$ como $\theta(\theta_1'\;\theta_2')'$

- La restricción es: $h(\theta)=\theta_2=0$

- En este caso, el estadístico de Wald es:

$$W=\hat{\theta}_2^2(N^{-1}\hat{C}_{22})^{-1}\hat{theta}_2$$

donde $N^{-1}\hat{C}_{22}=\hat{V}(\hat{\beta}_2)$

- $W$ se distribuye $\chi^2(h)$ bajo la $H_0$

- Este test es la generalización de la prueba $F$ que ustedes han usado ya

---

# Tests de restricciones no lineales

- Consideren que queremos probar la $H_0$ de que $\theta_1=\frac{1}{\theta_2}$, entonces:

$$H_0:\;h(\theta)=\frac{\theta_1}{\theta_2}-1=0$$
--

- En este caso, $R(\theta)=\left(\frac{\partial h(\theta)}{\partial \theta_1}\;\frac{\partial h(\theta)}{\partial \theta_2}\; 0\right)=\left(\frac{1}{\hat{\theta}_2}\;-\frac{\hat{\theta}_1}{\hat{\theta}_2^2}\; 0\right)$

- Noten que en este caso solo hay una hipótesis, por lo que $W$ es un escalar:

$$
W=
N\left(\frac{\theta_1}{\theta_2}-1\right)^2\left(\left(\frac{1}{\hat{\theta}_2}\;-\frac{\hat{\theta}_1}{\hat{\theta}_2^2}\; 0\right)
\begin{pmatrix}
\hat{c}_1^1 & \hat{c}_1^2 & \cdots  \\
\hat{c}_2^1 & \hat{c}_2^2 & \cdots  \\
\vdots & \vdots & \ddots \\
\end{pmatrix}
\left(\frac{1}{\hat{\theta}_2}\;-\frac{\hat{\theta}_1}{\hat{\theta}_2^2}\; 0\right)'\right)^{-1}
$$
- $W$ se distribuye asintóticamente $\chi^2(1)$ (por lo que $\sqrt{W}$ es $\mathcal{N}(0,1)$)

---

class: inverse, middle, center

# Método delta

---

# Método delta

- El método que usamos para derivar el test de Wald se usando aproximaciones de Taylor se conoce como **método delta** pues involucra las derivadas de $h(\theta)$

- Este método es más general y puede ser utilizado para obtener la distribución de una combinación no lineal de parámetros y hacer inferencia

- Por ejemplo, quisiéramos obtener $\frac{\hat{\theta}_1}{\hat{\theta}_2}$ y dar un intervalo de confianza para este cociente

---

# Método delta

- Supongamos que $\hat{\gamma}=h(\theta)$

- Y que sabemos que $\sqrt{N}(\hat{\theta}-\theta_0)\xrightarrow{d}\mathcal{N}(0,C_0)$

--

- El método delta implica hacer la expansión de Taylor alrededor de $\theta_0$ y resolver, para obtener:

$$\sqrt{N}(\hat{\gamma}-\gamma_0)\xrightarrow{d}\mathcal{N}(0,R_0C_0R_0')$$
- Y podemos estimar la variance de $\hat{\gamma}$ como $\hat{V}(\hat{\gamma})=\hat{R}N^{-1}\hat{C}\hat{R}$

--

- Podemos entonces estimar el error estándar $\hat{\gamma}$: $$se(\hat{\gamma})=\sqrt{\hat{r}N^{-1}\hat{C}\hat{r}'}$$


donde $r(\theta)=\partial\gamma/\partial\theta'=\partial h(\theta)/\partial\theta'$


---

# Ejemplo: exponencial

- Supongamos que estimamos $\beta$ de un modelo no lineal tal que podemos también estimar consistentemente la varianza de $\sqrt{N}(\hat{\beta}-\beta_0)$ como $\hat{C}$

- Supongamos que por alguna razón nos interesa conocer $\exp\{\beta\}=h(\beta)$

- En este caso $\partial h /\partial \beta=\exp\{\beta\}$

--

- Entonces, el error estándar será:

$$se(\exp\{\hat{\beta}\})=\sqrt{N^{-1}\exp\{\hat{\beta}\}^2\hat{V}(\hat{\beta})}=\exp\{\hat{\beta}\}se(\hat{\beta})$$



---

class: inverse, middle, center

# Tests basados en la verosimilitud

---

# Tests basados en la verosimilitud

- Se asume que la función de verosimilitud es conocida

- Queremos probar la hipótesis $h(\theta_0)=0$

- Para el test de razón de verosimilitud (LR) y para el test de multiplicador de Lagrange (LM), se requiere llevar a cabo la estimación con las restricciones:
  
  - $\hat{\theta}_u$: estimador no restringido
  - $\tilde{\theta}_r$: estimador restringido
  
--

- El estimador restringido $\tilde{\theta}_r$ maximiza el lagrangiano $\mathcal{L}(\theta)-\lambda'h(\theta)$, donde $\lambda$ es un vector de multiplicadores de Lagrange de dimension $h\times 1$

---

# Ejemplo: test de exclusión

- Supongamos $\theta=(\theta_1',\theta_2')'$ y queremos probar la hipótesis de exclusión de $h(\theta)=\theta_2=0$

- Entonces, $\tilde{\theta}_r=(\tilde{\theta}_{1r}',0')$

- $\tilde{\theta}_{1r}$ se obtiene maximizando con respecto a $\theta_1$ la log verosimilitud restringida $\mathcal{L}(\theta_1,0)$

---

# Test de razón de verosimilitud (LR)

- La intuición es que si la $H_0$ es verdadera, los máximos de la verosimilitud restringida y de la no restringida deberían ser los mismos

- Entonces, se usa una función de la diferencia entre $\mathcal{L}(\tilde{\theta}_r)$ y $\mathcal{L}(\hat{\theta}_u)$

--

- Definimos:

$$LR=-2(\mathcal{L}(\tilde{\theta}_r)-\mathcal{L}(\hat{\theta}_u))$$

- Se puede mostrar que bajo $H_0$
$$LM\sim\chi^2(h)$$


---

# Test de multiplicador de Lagrange

- Sabemos que en el máximo, $\frac{\mathcal{L}(\theta)}{\partial \theta}\Bigg|_{\hat{\theta}_u}=0$

- Si la $H_0$ es verdadera, entonces debe suceder que $\frac{\mathcal{L}(\theta)}{\partial \theta}\Bigg|_{\tilde{\theta}_r}\approx 0$

- Noten que esto implica diferencias en el score evaluado en distintos $\theta$, por lo que a este test también se le conoce como **test de scores**



---

# Próxima sesión

- Comenzaré con los modelos de variable categórica

  - CT Capítulo 16

- El jueves comenzamos con conteo

  - CT capítulo 20
---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**